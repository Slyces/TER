{% extends 'hello_world/base.html' %}
{% load extras %}
{% load staticfiles %}

{% block title %}
    <title>TER !</title>
{% endblock %}

{% block content %}
    <div class="main ui container">
        <div class="chapter content" id="fi">

            {# Grand titre #}
            <h1 class="chapter header">
                Applications pratiques
            </h1>

            <h2 class="chapter header">Introduction</h2>
            <div class="ui segment raised">
                <p>
                    Dans le cadre de notre TER, nous avons dans un premier temps cherché à nous approprier les concepts
                    fondamentaux concernant les réseaux de neurones artificiels, leurs applications en finance, et les
                    limites auxquelles ils restent confrontés.
                </p>
                <p>
                    Puis, nous avons décidé de coder nos propre réseaux de neurones artificiels afin de répondre
                    expérimentalement à la problématique suivante: « Peut-on prévoir efficacement les dynamiques
                    financières
                    en utilisant les réseaux de neurones artificiels ?».

                </p>
                <p>
                    Dans cette partie, nous présenterons dans un premier temps notre stratégie empirique concernant la
                    mise
                    en place de nos réseaux, puis nous discuterons des résultats que nous avons obtenus.
                </p>
            </div>

            <h2 class="chapter header">Stratégie empirique</h2>
            <div class="ui segment raised">
                <p>
                    Dans cette sous-partie, nous allons présenter nos choix des indices à prédire puis détailler le
                    codage
                    de nos réseaux de neurones artificiels.
                </p>

                <h3> 1 - Choix des indices à prédire </h3>
                <p>
                    Avant de nous attaquer à la programmation des réseaux de neurones artificiels à proprement parler,
                    nous
                    avons du effectuer un choix concernant les produits financiers dont nous allions cherché à prédire
                    les
                    dynamiques. Après une consultation avec notre tuteur, Olivier Brandouy, nous avons décidé de nous
                    intéresser à 3 indices boursiers : le S&P 500, le NASDAQ Composite et le Dow Jones Industrial
                    Average. Nous avons par la suite ajouté à cette liste le Federal Funds Rate.
                    En effet, ce sont des indices qui disposent tous d'une notoriété internationale, ce qui facilite
                    grandement la
                    collecte des données, et dont la prédiction s'avère par conséquent intéressante pour les
                    investisseurs.
                </p>

                <h4> A) S&P 500</h4>
                <p>
                    Le Standard & Poor's 500, connu sous l'abréviation S&P 500, est un indice boursier américain possédé
                    par
                    l'agence de notation Standard & Poor's. Il se base sur la capitalisation boursière, soit la valeur
                    de
                    marché de l'ensemble des actions d'une société, de 500 grandes entreprises américaines. Par
                    ailleurs,
                    le S&P 500 est un indice boursier relativement ancien, étant apparu en 1923. Il est alors connu sous
                    le
                    nom de Composite Index et ne regroupe qu'un faible nombre d'actions. L'indice boursier se développe
                    graduellement et atteint son apogée en 1957, où il regroupe les actions de 500 grandes entreprises :
                    il prend alors le nom de S&P 500.
                </p>
                <p>
                    Le S&P 500 se calcule selon la formule suivante:
                </p>
                <p>
                    <img class="alinea" src="{% static 'hello_world/images/sp500.gif' %}" alt="S&P 500"/>
                </p>
                <p>
                    Où $P_i$ représente le prix de l'action de l'entreprise $i$, $Q_i$ le nombre d'actions disponibles
                    publiquement,
                    et $Divisor$ une constante fixé par Standard & Poor's, approximativement égale à 8,9 milliards USD.
                </p>

                <h4> B) NASDAQ Composite</h4>
                <p>
                    Le National Association of Securities Dealers Automated Quotations ou NASDAQ est le second plus
                    grand
                    marché d'actions des Etats-Unis, derrière le New York Stock Exchange. Ouvert en 1971, il est le
                    premier
                    marché boursier électronique à être apparu et gardera ce statut pendant 15 ans. De plus, en terme de
                    volume, il cote à l'heure actuelle plus de 3200 valeurs, liées surtout aux entreprises de nouvelles
                    et
                    hautes technologies.
                </p>
                <p>
                    Son indice boursier, le NASDAQ Composite, est un des acteurs majeurs du marché d'action des
                    Etats-Unis.
                    Ce dernier est lancé en 1971, et cote alors les actions de 50 compagnies. Il connait une croissance
                    rapide, qui s'accélère dans les années 90 avec la bulle internet. En effet, le NASDAQ Composite
                    regroupe
                    de nombreuses entreprises liées à l'informatique, ce qui lui permet de bénéficier pleinement de
                    l'essor
                    des nouvelles technologies. L'indice atteint son apogée en mars 2000, avant de connaître un déclin
                    en
                    2002, suite à l'explosion de la bulle internet. Toutefois, il parvient à se redresser graduellement
                    et
                    atteint un nouveau record en 2015.
                </p>
                <p>
                    Contrairement au S&P 500 qui se limite à un nombre fixe d'entreprises cotées, le NASDAQ Composite
                    cote
                    l'ensemble des sociétés du NASDAQ, environ 3200. Chacune d'entre elles est affectées d'un poids
                    proportionnel à sa valeur de marché.
                </p>
                <p>
                    Le NASDAQ Composite se calcule selon la formule suivante :
                </p>
                <p>
                    <img class="alinea" src="{% static 'hello_world/images/nasdaq.gif' %}" alt="S&P 500"/>
                </p>
                <p>
                    Où $V_i$ est la valeur de marché de l'entreprise $i$ pondérée, $Divisor$ une constante déterminée
                    par le
                    NASDAQ.
                </p>

                <h4> C) Dow Jones Industrial Average</h4>
                <p>
                    Le Dow Jones Industrial Average est un des plus vieil indice boursier américain, établi pour la
                    première
                    fois en 1896, par Charles Dow. Ce dernier est le fondateur du Wall Street Journal, ainsi que de la
                    firme
                    éponyme  Dow Jones & Company,  avec son associé Edward Jones. Le Dow Jones Industrial Average est un
                    indice regroupant à l'origine les 30 plus grandes sociétés publiques américaines. En effet, nous y
                    trouvons alors des compagnies publiques appartenant au secteur industriel, telles que American Sugar
                    Company ou North American Company. Graduellement, avec le déclin de l'industrie et l'avénnement du
                    secteur tertiaire, les entreprises cotées au Dow Jones Industrial Average ont évolué. Ainsi, à
                    l'heure
                    actuelle, y figurent des compagnies telles que Apple, Coca-Cola, Microsoft, Nike ou McDonald's.
                </p>
                <p>
                    L'indice du Dow Jones Industrial Average se calcule selon la formule suivante :
                </p>
                <p>
                    <img class="alinea" src="{% static 'hello_world/images/dow.gif' %}" alt="Dow Jones"/>
                </p>
                <p>
                    Où $P_i$ est le prix de l'action de l'entreprise $i$ et $Dow Divisor$ une constante déterminée par
                    le Dow Jones Industrial Average.
                </p>
                <h4> d) Federal Funds Rate</h4>
                <p>
                    Finalement, nous nous sommes également intéressés au Federal Funds Rate, qui correspond au taux
                    d'intérêt auquel les banques américaines prêtent leurs réserves à d'autres institutions financières.
                </p>
                <h3> 2 - Programmation des réseaux de neurones artificiels </h3>

                <h4> A) Choix des plates-formes de programmation</h4>
                <p>
                    Après avoir déterminé les indices boursiers à prédire, nous avons débuté la programmation de nos
                    réseaux de neurones artificiels. Nous avons opté pour une double approche qui consiste à programmer,
                    dans un premier temps, un réseau de neurones artificiels en utilisant les librairies disponibles sur
                    le
                    logiciel R, puis à élaborer notre propre réseau, en utilisant le langage python et les librairies
                    dédiées aux machines learning. Cette double approche se justifie par notre volonté d'expérimenter la
                    conception des réseaux de neurones sur plusieurs plates-formes, afin d'établir une analyse
                    comparative.
                </p>

                <h4> B) Réseau de type feed-forward sur R</h4>
                <p>
                    Notre objectif principal lors de cette première étape est de programmer un réseau de neurones
                    artificiels sur R, de le visualiser, et d'établir des prédictions quant aux variations d'un indice
                    boursier,le S&P 500, que nous comparons ensuite aux prédictions obtenus grâce à un modèle ARIMA.
                </p>
                <p>
                    La programmation de notre réseau sur le logiciel R s'est faite grâce à l'interface graphique
                    RStudio.
                </p>

                <img src="{% static 'hello_world/images/R1.jpg' %}" alt="Importation des librairies"/>

                <p>
                    Dans un premier temps, nous avons importé les 4 librairies nécessaires à notre entreprise : tseries,
                    forecast, neuralnet, et Hmisc. Tseries est une librairies permettant entre autres de modéliser une
                    série
                    chronologique par un modèle ARIMA, forecast permet d'établir des prédictions sur ce modèle,
                    neuralnet de
                    construire notre réseau de neurones artificiels et Hmisc de classer les données d'un tableau avec un
                    cran de retard ou lag. Ces librairies disposent de nombreuses autres fonctionnalités que nous
                    n'avons
                    pas utilisées.
                </p>
                <p>
                    Après avoir importé les librairies, nous nous sommes attaché à la récupération et au traitement des
                    données de nos indices boursiers. Dans un premier temps, nous avons téléchargé les données relatives
                    à
                    l'indice S&P 500, entre le 1er janvier 2013 et le 30 décembre 2016, sous format csv, depuis le site
                    https://fr.finance.yahoo.com/. Nous obtenons alors le tableau partiellement représenté ci-dessous :
                </p>

                <img class="image center" align="center" src="{% static 'hello_world/images/R6.jpg' %}"
                     alt="S&P 500"/>

                <p>
                    Nous y trouvons 1008 observations caractérisées par 7 variables : la date, le cours d'ouverture, le
                    cours
                    maximal atteint dans la journée, le cours minimal atteint dans la journée, les cours de fermeture,
                    et le
                    volume échangé. Seules la variable cours d'ouverture, Open, nous intéresse. En effet, nous avons
                    arbitrairement décidé de nous baser sur celui-ci lors de nos calculs et de nos prédictions.
                </p>

                <img src="{% static 'hello_world/images/R2.jpg' %}" alt="Traitement des données"/>

                <p>
                    Une fois les données importées, nous stockons les 1000 premières observations des cours d'ouvertur
                    dan une variable nommée X, puis nous les convertissons en type numeric. Nous utilisons ensuite l
                    fonctio tsclean afin de compléter les données manquantes. Nous stockons ensuite les 1001 à 1008 èm
                    observation dans la variable true.pred.X, à laquelle nous appliquons le même traitement que X. Cette
                    variabl représente les données réelles que nous chercherons à prédire grâce au modèle ARIMA et à
                    notr réseau d neurones artificiels
                </p>

                <img src="{% static 'hello_world/images/R3.jpg' %}" alt="Auto.arima"/>

                <p>
                    Par la suite, après le traitement de nos données, nous les modélisons grâce à la fonction auto.arima
                    et nous obtenons nos prédictions grâce à la fonction predict
                </p>

                <img src="{% static 'hello_world/images/R4.jpg' %}" alt="Normalisation"/>

                <p>
                    Nous nous attaquons ensuite à la conception de notre réseau de neurones artificiels. Premièrement il
                    nous faut procéder à la normalisation de nos données. Pour se faire, nous enlevons à chaque
                    observation le minimum de X et nous le divisons par le maximum moins le minimum. Nous stockons les
                    observation normalisées dans le vecteur Xnorm. Nous séparons ensuite ce vecteur en deux : train, qui
                    représente l'échantillon sur lequel nous allons entraîner le réseau, et test, qui représente
                    l'échantillon su lequel nous allons le tester.
                </p>
                <p>
                    Par la suite, train et test sont convertis en objets ts (time series) et nous créons 2 tableaux
                    datatrain et datatest. Ces tableaux contiennent à la première colonne les vecteurs tests,
                    respectivement train, à la seconde colonne les valeurs de test, respectivement train, au jour
                    précédent (t-1), et à la troisième colonne les valeurs de test, respectivement train, au jour
                    d'avant (t-2). En effet, nous avons choisi 2 paramètres d'entrée, x1 et x2 pour notre réseau de
                    neurones artificiels : le cours de l'indice au jour précédent, l'instant t-1, et le cours au jour
                    d'avant, l'instant t-2. De manière intuitive, ce choix s'explique par la volonté d'imiter un modèle
                    ARIMA, en supposant que le cours d'un indice boursier au jour j dépend en partie de sa valeur
                    d'hier, j-1, et d'avant hier, j-2, d'où la nécessité de regrouper ces trois variables dans un même
                    tableau.
                </p>
                <p>
                    Nous établissons ensuite la formule sur laquelle va travailler notre réseau de neurone :
                    Xnorm~x1+x2. Le réseau va donc chercher à expliquer la valeur de Xnorm à l'instant t en fonction de
                    Xnorm à l'instant t-1 (x1) et Xnorm à l'instant t-2 (x2). Cette formule est stockée dans la variable
                    f.
                </p>
                <p>
                    Par la suite, nous procédons à la création du réseau, noté rn, grâce à la fonction neuralnet qui
                    permet de créer un réseau de neurones artificiels et de l'entrainer. Cette fonction prend comme
                    paramètres la formule f, notre échantillon d'entrainement, datatrain, et le nombre de couches
                    cachées, ici une couche de 50 neurones. Notre réseau est donc, dans ce cas de figure, un réseau de
                    type Feed Forward à une couche.
                </p>
                <p>
                    Nous procédons ensuite à la phase de prédiction, grâce à la fonction compute, qui prend en entrée
                    notre réseau de neurones entrainé, rn, ainsi que les colonnes x1 et x2 de notre echantillon test,
                    datatest. Les données prédites sont stockées dans le vecteur Xpred que nous dénormalisons pour
                    finalement obtenir le vecteur Xpreddenorm.
                </p>

                <img src="{% static 'hello_world/images/R5.jpg' %}" alt="Traitement des données"/>

                <p>
                    Finalement, pour comparer notre réseau au modèle ARIMA, nous effectuons une prédiction sur les 8
                    prochains jours, à partir du 1000ème jour, en alimentant les x1 et les x2 du réseau non plus par les
                    valeurs réelles de x1 et x2, mais par les valeurs prédites par le réseau aux instants t-1 et t-2.
                    Les raisons de cette démarche seront expliquées dans la partie résultat.
                </p>

                <h4> C) Réseau de type feed forward sur Python </h4>

                <p>
                    Dans cette partie, nous nous sommes inspirés, suite aux conseils de notre tuteur, de <a
                        style="color: black;"
                        href="https://www.codeproject.com/articles/175777/financial-predictor-via-neural-network">l'article
                    suivant</a>. Nous avons choisi d'implémenter un réseau sur Python, après l'implémentation R. Si R
                    est le langage privilégié des statistiques, Python est celui du Machine Learning. Ainsi, c'est sur
                    Python que l'on trouve les librairies les plus puissantes et les plus complètes, qui se développent
                    d'autant plus en ce moment.
                </p>
                <p>
                    Nous avons ainsi choisi d'utiliser la librairie Tensorflow créée et entretenue par google, l'un des
                    acteurs majeurs du machine learning, pour modéliser un réseau de neurones en
                    feed-forward inspiré de l'article sus-cité. Cette approche, contrairement à celle de R, offre
                    beaucoup plus de flexibilité dans la création du réseau, et nous permet de mieux maitriser tous les
                    points qui peuvent varier.
                </p>
                <p>
                    Le réseau proposé dans l'article se résume par les informations suivantes :
                </p>

                <ol class="ui list">
                    <li value="*">
                        Objectif : prédire le S&P500, le Nasdaq, le DowJones et les Taux d'intêrets, soit $4$ variables
                        en
                        sortie.
                    </li>
                    <li value="*">Prédictions : basées sur les 10 derniers jours, soit $4 &times; 10 = 40$ données</li>
                    <li value="*">Structure : [40 x 41 x 41 x 4] c'est à dire deux hidden-layers de 41 neurones.</li>
                    <li value="*">Fonction d'activation : tangente hyperbolique</li>
                    <li value="*">Algorithme d'apprentissage : resilient propagation training</li>
                    <li value="*">Normalisation : Soit $X$ l'ensemble des valeurs d'une variable, $∀ x ∈ X, f(x) = (x -
                        min(X)) / (max(x) - min(x))$
                    </li>
                </ol>

                <p>
                    Revenons sur certains points ci-dessus :
                </p>
                <p>
                    <strong> Prédictions </strong><br/>
                    Nous avons décidé de suivre la méthode de prédiction, et pour cela nous avons téléchargé les données
                    fournies dans l'article (écrit dans un langage de programation que nous ne connaissons pas). Ces
                    données, 4 fichiers distincts au format csv, ressemblent à ceci :
                </p>

                <img align="center" src="{% static 'hello_world/images/nasdaq_excel.png' %}" alt="Nasdag datas">

                <p>
                    Nous nous sommes vite rendu compte de petites incohérences dans les données, tel que la présence de
                    différent champs pouvant être utilisés, des nombre différents de données et la non-homogénéité des
                    dates . Nous avons donc commencé par harmoniser ces données, les re-disposer dans l'ordre
                    chronologique, ne conserver que les dates présentes dans les 4 fichiers en même temps et finalement
                    les normaliser.
                </p>
                <p>
                    <strong>Normalisation et fonction d'activation</strong><br/>
                    La normalisation est dans l'intervalle [0, 1]. La fonction d'activation tangente hyperbolique est
                    dans [0, 1] pour tout donnée entrante positive. Or nos données normalisées sont dans [0, 1], où 0
                    est le minimum de la variable sur la période et 1 est le maximum. Par récurrence, on voit très bien
                    que le réseau restera dans [0, 1].
                </p>
                <p>
                    <strong>Algorithme d'apprentissage</strong><br/>
                    Le resilient propagation training est un algorithme de backpropagation, ou rétropropagation, qui
                    ressemble beaucoup à l'algorithme de
                    descente du gradient. La différence ici est qu'au lieu d'avoir un pas
                    d'apprentissage qui décide de l'ampleur de la correction effectuée sur les poids en fonction de la
                    valeur du gradient, nous n'utilisons que la direction du gradient et pas sa valeur.<br/>
                    Cependant, l'algorithme en question n'était pas disponible dans Tensorflow, nous avons utilisé un
                    algorithme de descente du gradient classique.
                </p>
                <p>
                    <strong>Construction du réseau</strong><br/>
                    Contrairement à la librairie utilisée dans R, Tensorflow est une librairie qui se veut beaucoup plus
                    générale. Elle est donc très puissante et flexible, mais demande un certain nombre d'instructions
                    pour créer un modèle fonctionnel. Nous allons donc passer assez vite sur le code, en introduisant
                    quelques concepts simples. La première étape est de construire le graphe computationel, c'est à dire
                    un graphe des calculs à effectuer. La seconde étape, strictement séparée, sera d'utiliser ce graphe
                    dont la structure est figée pour effectuer des calculs et modifier certains paramètres du réseau :
                    les poids. Le code suivant illustre la création des étapes de calcul qui fournissent une prédiction.
                </p>
                {#                <div class="ui segment raised">#}
                <pre class="prettyprint">
def model(X, w_h1, w_h2, w_o): # Respectivement : données entrantes, poids de la première couche,
    h1 = tf.nn.tanh(tf.matmul(X, w_h1))          # poids de la seconde couche, poids de la sortie
    h2 = tf.nn.tanh(tf.matmul(h1, w_h2))
    return tf.nn.tanh(tf.matmul(h2, w_o))
                    </pre>
                {#                </div>#}
                <p>
                    Nous avons ici deux couches profondes, h1 et h2 chacune associées à leur poids (weights). Ainsi, h1
                    par exemple est la fonction d'activation appliquée au résulat de la multiplication matricielle de la
                    couche précédente, soit X, l'input, avec ses poids associés w_1. De même, la prédiction a des poids
                    associés (weights_out). Chacune des instructions avec <strong>tf.</strong> créent un noeud dans le
                    graphe qui effectuera une opération.
                </p>
                <p>
                    Ici, X prendra sera remplacé par les données entrantes, et tous les autres paramètres sont des
                    matrices variables, c'est à dire que le but de l'apprentissage sera d'adapter leurs valeurs pour
                    adapter la prédiction.
                </p>
                <p>
                    <strong>Optimisation</strong><br/>
                    L'optimisation se fait également avec un noeud du graphe :
                </p>
                <pre class="prettyprint">
X = tf.placeholder("float", [None, 40])
Y = tf.placeholder("float", [None, 4])

w_h = init_weights([40, 41])
w_h2 = init_weights([41, 41])
w_o = init_weights([41, 4])

mdl = model(X, w_h, w_h2, w_o) # On construit le modèle avec X l'input externe

cost = tf.reduce_sum(tf.square(mdl - Y)) # Fonction de coût
train_op = f.train.GradientDescentOptimizer(0.001).minimize(cost) # Minimisation du coût
                </pre>
                <p>
                    Ici, on remarque que <strong>placeholder</strong> montre les emplacement de données externes. On
                    crée ensuite une fonction de coût, qui doit estimer la réussite de notre modèle. Nous utilisons la
                    définition classique de l'erreur sur ce premier jet inspiré de l'article. Il suffit ensuite d'une
                    instruction pour utiliser une descente du gradient de pas d'apprentissage 0.001 qui minimisera
                    l'erreur.
                </p>
                <p>
                    <strong>Entraînement du réseau</strong><br/>
                    Pour entraîner le réseau, il suffit d'effectuer le calcul du noeud d'optimisation avec comme entrées
                    pour X et Y les données d'entraînement. La prédiction sera faite tout aussi simplement en exécutant
                    le noeud créé par la fonction build_model avec comme X les données de test.
                </p>
                <p>
                    Nous avons ensuite beaucoup expérimenté sur ce modèle, en essayant diverses fonctions
                    d'optimisations, d'activation, tailles de réseaux et valeurs du pas d'apprentissage.
                </p>
                <p>
                    <strong>Sauvegarde</strong><br/>
                    Nous avons essayé d'approfondir un peu l'utilisation de ce modèle en essayant de créer une structure
                    pour sauvegarder ses paramètres d'une utilisation à l'autre.
                </p>

                <h4> D) Réseau de type Long/Short Term Memory sur Python </h4>

                <p> Nous avons décidé de faire une autre implémentation sur python, d'un nouveau réseau. Ce réseau est
                    de
                    type réseau récurrent, plus précisément Long/Short Term Memory, c'est à dire un réseau qui réutilise
                    dans ses calculs ses résultats précédents. En somme, ce réseau a la particularité d'être très
                    adaptés
                    aux séquences ordonnées. Notre problème rentre particulièrement bien dans ce cadre. Manquant de
                    temps,
                    nous n'avons cependant pas beaucoup exploité ce réseau.
                </p>
                <p>
                    Il a été réalisé avec une librairie qui vient se rajouter au dessus de Tensorflow pour le simplifier
                    :
                    Keras. Ainsi, le fonctionnement est très lié à celui de la librairie avec laquelle nous avons déjà
                    expérimenté.
                </p>
                <p>
                    <strong>Le modèle</strong><br/>
                    Ce modèle utilise comme seule donnée les valeurs antérieures du système. Il est déjà évident que
                    cela ne
                    suffira pas à obtenir de bon résultats. Pour avoir quelque chose de plus complet, il faudrait soit
                    coupler plusieurs données des marchés financiers, soit s'intéresser à des données
                    extra-financières.<br/>
                    Le modèle de ce réseau est créé par le code suivant :

                </p>
                <pre class="prettyprint">
def build_model(layers, drop=0.2):
    model = Sequential()
    model.add(LSTM(
        input_dim=layers[0], # Ici 1
        output_dim=layers[1], # Ici 50
        return_sequences=True)) # revoie aux autres layers
    model.add(Dropout(drop))
    model.add(LSTM(
        layers[2], # Ici 100
        return_sequences=False))
    model.add(Dropout(drop))
    model.add(Dense(
        output_dim=layers[3])) # Ici 1
    model.add(Activation("linear"))
    start = time.time()
    model.compile(loss="mse", optimizer="rmsprop")
    print("Compilation Time : ", time.time() - start)
    return model
                </pre>
                <p>
                    Ce modèle fonctionne comme suit :<br/>
                    Le premier layer reçoit une unique donnée en entrée. Elle
                </p>
            </div>

            <h2 class="chapter header">Résutats Obtenus</h2>

            <div class="ui segment raised">
                <h3> 1 - Résultats obtenus par les réseaux de neurones artificiels programmés sur R</h3>
                <p>
                    <img class="image center" src="{% static 'hello_world/images/resultr1.jpg' %}" alt="Résultat"
                         style="width:800px;height:600px;"/>
                </p>
                <p>
                    <img class="image center" src="{% static 'hello_world/images/resultr2.jpg' %}" alt="Résultat"
                         style="width:800px;height:600px;"/>
                </p>
                <p>
                    <img class="image center" src="{% static 'hello_world/images/resultr3.jpg' %}" alt="Résultat"
                         style="width:800px;height:600px;"/>
                </p>
                <p>
                    <img class="image center" src="{% static 'hello_world/images/resultr4.jpg' %}" alt="Résultat"/>
                </p>
                <div class="legend">Représentation graphique du réseau de neurones artificiels obtenus sur R</div>

                <p>
                    les 3 premiers graphiques représentent les mêmes données avec un degrès de zoom différent. La courbe
                    en
                    bleue
                    représente le cours réel du S&P 500 entre le premier et le 1000ème jour, celle en pointillés noirs
                    le
                    cours réel du S&P 500 entre le 1000ème et le 1008ème jour, celle en rouge la modélisation du modèle
                    ARIMA obtenu par auto.arima, celle en pointillés oranges la prédiction du modèle ARIMA, celle en
                    vert
                    la modélisation du réseau de neurones sur x1 et x2 réels, et celle en pontillé bleus la modélisation
                    du
                    réseau sur x1 et x2 prédits.
                </p>

                <p>
                    Nous observons premièrement, que notre modèle ARIMA est de la forme ARIMA(1,1,0). En effet, son
                    paramètre p est égale à 1, son degrès de différenciation d est égale à 1, et son son paramètre q est
                    égal à 0. L'AIC du modèle est de 8180,3. La modélisation ARIMA est, graphiquement, satisfaisante, de
                    même que ses prédictions qui sont relativement proches des données réelles. En effet, grahiquement,
                    les
                    courbes en pointillés oranges et noirs présentent un écart relativement faible.
                </p>

                <p>
                    En ce qui concerne la modélisation faite par le réseau de neurones artificiels nous observons 2
                    courbes,
                    l'une en verte, l'autre en pointillés bleus. La courbe en verte représente la modélisation du réseau
                    de
                    neurones artificiels sur l'échantillon test. A première vue, nous pourrions croire que cette
                    modélisation est très satisfaisante, étant donnée que, graphiquement, la courbe verte, représentant
                    la
                    modélisation par le réseau, et la courbe en bleu, représentant les données réelles, sont très
                    proches au
                    niveau de leur allure. Toutefois, cette conclusion est érronnée. En effet, la modélisation du réseau
                    correspondant à la courbe en verte s'est faite point après point, à partir des x1 et x2 réels.
                    Ainsi,
                    à l'instant t, le réseau prédit la valeur du cours en se basant sur sa valeur réelle à l'instant t-1
                    et
                    t-2. A l'instant t+1, le réseau prédit la valeur du cours en se basant sur la valeur réelle à
                    l'instant
                    t (et non celle qu'il a prédite à l'instant t) et à l'instant t-1. Par conséquent, lorsque le réseau
                    prédit la valeur du cours il commet une erreur, mais cette erreur est automatiquement compensée lors
                    de
                    la prédiction suivante puisque ce dernier utilise les données réelles et non les données prédites.
                </p>

                <p>
                    Afin de tester le véritable pouvoir prédictif de notre réseau, nous devons donc faire en sorte que
                    sa
                    prédiction du cours à l'instant t+1 se base sur la valeur prédite à l'instant t, et non sur la
                    valeur
                    réelle. La courbe en pointillé bleue correspond à cette approche. Nous constatons immédiatement,
                    graphiquement, que les prédictions du réseau de neurones sont beaucoup plus éloignées des données
                    réelles que la prédiction ARIMA. En effet, les courbes en pointillés noirs et bleus présentent un
                    écart
                    important, croissant temporellement. De plus, la courbe représentant la prédiction du réseau tend à
                    décroitre de manière uniforme. Ce résultat s'explique par le fait que lorsque le réseau effectue une
                    prédiction à l'instant t, il commet une erreur e. Lors de la prédiction suivante à l'instant t+1, il
                    se
                    base sur la prédiction érronée de l'instant t, et commet encore une autre erreur e1. Il existe donc
                    un
                    mécanisme d'amplification des erreurs commise par le réseau, ce qui explique le fait que plus nous
                    nous
                    éloignons dans le temps, plus la prédiction faite par le réseau est éloignée des données réelles.
                </p>

                <p>
                    En conclusion, sur cette application pratique, le modèle ARIMA donne de meilleurs prédictions que
                    notre
                    réseau de neurones artificiels. Nous avons expérimentés plusieurs types de réseaux, en faisant
                    varier le
                    nombre de couches cachées et le nombre de neurones les composant, mais nous n'avons pas obtenus de
                    résultats significativement différents. Le réseau de neurones artificiels semble donc présenter,
                    dans le
                    cas considéré, un certain intérêt pour les prédictions à court terme, d'un jour sur l'autre par
                    exemple,
                    mais perd rapidement son pouvoir prédictif à long terme, contrairement au modèle ARIMA. Toutefois,
                    cette
                    conclusion ne fait pas office de généralisation. En effet, notre réseau de neurones artificiels
                    reste
                    relativement simple dans son architecture, et nous l'avons testé sur un seul échantillon de 1008
                    données.
                    De plus, il n'est pas exclu que nous ayons commis des erreurs de paramétrages du réseau.
                </p>

                <p>
                    Au vu de cette première expémentation, nous formulons donc l'hypothèse qu'à court terme les réseaus
                    de neurones artificiels donnent des prédictions légèrement meilleurs que ceux obtenus de manière
                    aléatoire, mais qu'ils perdent rapidement de leur pouvoir prédictif à long terme. Afin de confirmer
                    ou d'infirmer cette hypothèse, nous avons décidé de pogrammer un réseau de neurones artificiels en
                    utilisant un autre logiciel, python.
                </p>

                <h3> 2 - Résultats obtenus par les réseaux de neurones artificiels programmés sur Python</h3>
                <p>
                    Nos réseaux de neurones programmés en python, contrairement aux réseaux programmés en R, se basent
                    sur les 10 dernières données historiques passées afin d'établir une prédiction à l'instant t. Nous
                    observons que cette approche donne des résultats satisfaisants pour le réseau de neurones Feed
                    Forward.
                </p>
                <p>
                    En effet, pour les 4 produits financiers, la courbe représentant les données prédites et celle
                    représentant les données historiques sont très proches dans leur allure. De plus, si nous observons
                    les erreurs, qui correspondent à la différence entre les données prédites et les données
                    historiques,
                    nous remarquons que pour les 3 indices boursiers, S&P 500, NASDAQ Composite et Dow Jones
                    Industrial, elles sont infèrieures ou égales à 1%, ce qui atteste de la qualité de notre
                    modélisation. Néanmoins,
                    pour le Rates, la modélisation est beaucoup moins bonne. En effet, nous observons des erreurs allant
                    jusqu'à
                    25%. Pour les 3 indices boursiers, nous pouvons donc prédire à court terme avec relativement de
                    précision.
                </p>
                <p>
                    Toutefois, ces résultats satisfaisants s'expliquent par le fait que nous prédisons point par point,
                    en nous basant
                    uniquement sur les données réelles. Ainsi, chaque erreur commise lors d'une prédiction à l'instant
                    t n'est pas prise en compte lors de la prédiction suivante à l'instant t+1, puisqu'elle se base
                    uniquement sur
                    les données historiques. Il existe donc un mécanisme d'auto-correction des erreurs. Afin de tester
                    le réel
                    pouvoir prédictif de notre réseau sur le long terme, il aurait fallu effectuer des prédictions sur
                    sur une plage de
                    donnée, en alimentant notre modèle non plus par des données historiques mais par les données passées
                    qu'il aurait prédit.
                    Cependant, faute de temps, nous n'avons pas pu mener cette approche en Python. A l'heure actuelle,
                    notre réseau de neurones
                    peut donc être qualifié de court-terme uniquement.
                </p>


                <div id="context1">
                    <div class="ui pointing secondary four item indexes menu">
                        <a class="active blue item" data-tab="Rates">Rates</a>
                        <a class="blue item" data-tab="DowJones">DowJones</a>
                        <a class="blue item" data-tab="Nasdaq">Nasdaq</a>
                        <a class="blue item" data-tab="SnP500">SnP500</a>
                    </div>
                    {% for index in indexes %}
                        <div class="ui tab {{ index|active }}" data-tab="{{ index }}">
                            <h1 class="header" align="center" style="margin-top: -1em;">{{ index }}</h1>
                            <div class="ui three item menu plots">
                                <a class="active item blue" data-tab="plot {{ index }}">Echelle réelle</a>
                                <a class="item green" data-tab="normal {{ index }}">Normalisé</a>
                                <a class="item orange" data-tab="error {{ index }}">Erreur</a>
                            </div>

                            <div class="ui active tab segment raised plt" data-tab="plot {{ index }}">
                                <p> {{ plot|key:index|safe }} </p>
                            </div>
                            <div class="ui tab segment raised plt" data-tab="error {{ index }}">
                                <p> {{ error|key:index|safe }} </p>
                            </div>
                            <div class="ui tab segment raised plt" data-tab="normal {{ index }}">
                                <p> {{ normalised|key:index|safe }} </p>
                            </div>
                        </div>
                    {% endfor %}
                </div>

                <div id="context2">
                    <h1 class="header" align="center">Long/Short Term Memory</h1>

                    <div class="ui two item menu longshort">
                        <a class="active item" data-tab="blblbl lstm">Echelle réelle</a>
                        <a class="item" data-tab="bibibibi lstm">Erreur</a>
                    </div>

                    <div class="ui active tab segment raised plt" data-tab="blblbl lstm">
                        <p> {{ plot|key:'lstm'|safe }} </p>
                    </div>
                    <div class="ui tab segment raised plt" data-tab="bibibibi lstm">
                        <p> {{ error|key:'lstm'|safe }} </p>
                    </div>
                </div>

                <img class="image center" src="{% static 'hello_world/images/poly_lstm.png' %}">
            </div>
        </div>
        {{ script|safe }}
    </div>
{% endblock %}
